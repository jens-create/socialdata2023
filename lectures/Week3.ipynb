{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "I hope you're getting the hang of things. Today we're going on with the prinicples of data visualization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This week, we will work our way through four parts:\n",
    "\n",
    "* First a lecture. You will watch a video on visualization and solve a couple of exercises.\n",
    "* Second, some reading. We'll be reading about *scientific data visualization*, and the huge number of things you can do with just one variable. Naturally, we'll be answering questions about that book. \n",
    "* Third, we'll be reproducing some of the plots from that book.\n",
    "* And finally, we will get you going -- hands on -- with plotting geodata using Plotly. This is warming up for next time, when we'll dig deeper into geo-viz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little bit of admin\n",
    "\n",
    "I will be posting assignment 1 during lecture 4. At that time, I'll talk about how to hand it (where to upload) and how we use peer evaluations in the course. But if you want to get started on that part already today, you can check out the video below. \n",
    "\n",
    "Some of you like to have this information early, so I'm posting it today. But **this admin part is not crucial to know about until next time, so you can safely skip if you're not interested**.\n",
    "\n",
    "[![VIDEO](https://img.youtube.com/vi/-TC18KgpiIQ/0.jpg)](https://www.youtube.com/watch?v=-TC18KgpiIQ)\n",
    "\n",
    "And after watching the video, you should make sure you're familiar with this page on the Wiki https://github.com/suneman/socialdata2023/wiki/Evaluations-vs-Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fundamentals of data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we had a small introduction of data visualization. Today, we are going to be a bit more specific on data analysis and visualization. Digging a bit more into the theory with the next video.\n",
    "\n",
    "<mark>*You may feel tempted to skip the lectures on dataviz, but they are quite important. We don't have a formal book on data visualization. So the only source of knowledge about the **principles**, **theories**, and **ideas**, that are the foundation for good data viz, comes from the videos*. So **watch them** ðŸ¤“ </mark>\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/yiU56codNlI/0.jpg)](https://www.youtube.com/watch?v=yiU56codNlI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Excercise:* Questions for the lecture\n",
    "> * As mentioned earlier, visualization is not the only way to test for correlation. We can (for example) calculate the Pearson correlation. Explain in your own words how the Pearson correlation works and write down it's mathematical formulation. Can you think of an example where it fails (and visualization works)?\n",
    "> * What is the difference between a bar-chart and a histogram?\n",
    "> * I mention in the video that it's important to choose the right bin-size in histograms. But how do you do that? Do a Google search to find a criterion you like and explain it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Reading about the theory of visualization\n",
    "\n",
    "Since we can go deeper with the visualization this year, we are going to read the first couple of chapters from [*Data Analysis with Open Source Tools*](http://shop.oreilly.com/product/9780596802363.do) (DAOST). It's pretty old, but I think it's a fantastic resource and one that is pretty much as relevant now as it was back then. The author is a physicist (like Sune) so he likes the way he thinks. And the books takes the reader all the way from visualization, through modeling to computational mining. Anywho - it's a great book and well worth reading in its entirety. \n",
    "\n",
    "As part of this class we'll be reading the first chapters. Today, we'll read chaper 2 (the first 28 pages) which supports and deepens many of the points we made during the video above. \n",
    "\n",
    "To find the text, you will need to go to **DTU Learn**. It's under \"Course content\" $\\rightarrow$ \"Content\" $\\rightarrow$ \"Reading\" $\\rightarrow$ `DAOST_chapter1.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Excercise*: Questions for DAOST\n",
    "> * Explain in your own words the point of the jitter plot.\n",
    "> * Explain in your own words the point of figure 2-3. (I'm going to skip saying \"in your own words\" going forward, but I hope you get the point; I expect all answers to be in your own words).\n",
    "> * The author of DAOST (Philipp Janert) likes KDEs (and think they're better than histograms). And we don't. Sune didn't give a detailed explanation in the video, but now that works to our advantage. We'll ask you to think about this and thereby create an excellent exercise: When can KDEs be misleading? \n",
    "> * Sune discussed some strengths of the CDF - there are also weaknesses. Janert writes \"CDFs have less intuitive appeal than histograms of KDEs\". What does he mean by that?\n",
    "> * What is a *Quantile plot*? What is it good for. \n",
    "> * How is a *Probablity plot* defined? What is it useful for? Have you ever seen one before?\n",
    "> * One of the reasons we like DAOST is that Janert is so suspicious of mean, median, and related summary statistics. Explain why one has to be careful when using those - and why visualization of the full data is always better. \n",
    "> * Sune loves box plots (but not enough to own one of [these](https://images.app.goo.gl/rpozyRX3xu5oFobt8) ðŸ˜‚). When are box plots most useful?\n",
    "> * The book doesn't mention [violin plots](https://en.wikipedia.org/wiki/Violin_plot). Are those better or worse than box plots? Why?\n",
    "> * Remember the box-plot part from [this video from last time](https://www.youtube.com/watch?v=DbJyPELmhJc) (the part that starts at 0:56)? Explain in your own words how this video illustrates potential issues even with box-plots? Do violin-plots help with that issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualizations based on the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Excercise Part 1*: Connecting the dots and recreating plots from DAOST but using our own favorite dataset.\n",
    "> * Let's make a jitter-plot (that is, code up something like **Figure 2-1** from DAOST from scratch), but based on *SF Police data*. My hunch from inspecting the file is that the police-folks might be a little bit lazy in noting down the **exact** time down to the second. So choose a crime-type and a suitable time interval (somewhere between a month and 6 months depending on the crime-type) and create a jitter plot of the arrest times during a single hour (like 13-14, for example). So let time run on the $x$-axis and create vertical jitter.\n",
    "> * Last time, we did lots of bar-plots. Today, we'll play around with histograms (creating two crime-data based versions of the plot-type shown in DAOST **Figure 2-2**). I think the GPS data could be fun to see this way. \n",
    ">   * This time, pick two crime-types with different geographical patterns **and** a suitable time-interval for each (you want between 1000 and 10000 points in your histogram)\n",
    ">   * Then take the latitude part of the GPS coordinates for each crime and bin the latitudes so that you have around 50 bins across the city of SF. You can use your favorite method for binning. I like `numpy.histogram`. This function gives you the counts and then you do your own plotting. \n",
    "> * Next up is using the plot-type shown in **Figure 2-4** from DAOST, but with the data you used to create Figure 2.1. There is not a single great way to create kernel density plots in Python. [Here](https://www.geeksforgeeks.org/density-plots-with-pandas-in-python/), you'll finde a pandas based strategy, but you can also use `gaussian_kde` from `scipy.stats` ([for an example, check out this stackoverflow post](https://stackoverflow.com/questions/4150171/how-to-create-a-density-plot-in-matplotlib)) or you can use [`seaborn.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html). There is also another nice tutorial for KDE plots [here](https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0).\n",
    "> * Now grab 25 random timepoints from the dataset (of 1000-10000 original data) you've just plotted and create a version of Figure 2-4 based on the 25 data points. Does this shed light on why I think KDEs can be misleading? \n",
    ">\n",
    "> Let's take a break. Get some coffee or water. Stretch your legs. Talk to your friends for a bit. Breathe. Get relaxed so you're ready for the second part of the exercise. \n",
    ">\n",
    "> \n",
    "> *Excercise Part 2*:\n",
    ">\n",
    "> * Now we'll work on creating two versions of the plot in **Figure 2-11**, but using the GPS data you used for your version of Figure 2-2. Comment on the result. It is not easy to create this plot from scracth.    \n",
    ">   * **Hint:** Take a look at the `scipy.stats.probplot` function. \n",
    "> * OK, we're almost done, but we need some box plots. Here, I'd like you to use the box plots to visualize fluctuations of how many crimes happen per day. We'll use data from the 15 focus crimes defined last week.\n",
    ">   * For the full time-span of the data, calulate the **number of crimes per day** within each category for the entire duration of the data.\n",
    ">   * Create a box-and whiskers plot showing the mean, median, quantiles, etc for all 15 crime-types side-by-side. There are many ways to do this. I like to use [matplotlibs's built in functionality](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html), but you can also achieve good results with [seaborn](https://seaborn.pydata.org/generated/seaborn.boxplot.html) or [pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html).\n",
    ">   * What does this plot reveal that you can't see in the plots from last time?\n",
    "> * Also I want to show you guys another interesting use of box plots. To get started, let's calculate another average for each focus-crime, namely what time of day the crime happens. So this time, the distribution we want to plot is the average time-of-day that a crime takes place. There are many ways to do this, but let me describe one way to do it. \n",
    ">   * For datapoint, the only thing you care about is the time-of-day, so discard everything else.\n",
    ">   * You also have to deal with the fact that time is annoyingly not divided into nice units that go to 100 like many other numbers. I can think of two ways to deal with this.\n",
    ">     * For each time-of-day, simply encode it as seconds since midnight.\n",
    ">     * Or keep each whole hour, and convert the minute/second count to a percentage of an hour. So 10:15 $\\rightarrow$ 10.25, 8:40 $\\rightarrow$ 8.67, etc.\n",
    "> * Now you can create box-plots to create an overview of *when various crimes occur*. Note that these plot have quite a different interpretation than ones we created in the previous exercise. Cool, right?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing geodata with Plotly\n",
    "\n",
    "So visualizing geodata used to be difficult, but with `Plotly` things have gotten easier. \n",
    "\n",
    "Like matplotlib, Plotly is an [open-source data visualization library](https://plotly.com/python/), but it's aimed at making interactive visualizations that can be rendered in a web browser (or jupyter notebook). You can read about it and learn how to install it [here](https://plotly.com/python/getting-started/).\n",
    "\n",
    "That means that we can easily draw on the fact that the crime data has lots of exciting geo-data attached. The map we're going to be creating is called a **[choropleth map](https://en.wikipedia.org/wiki/Choropleth_map)** (more on these later), which is basically a map, where we color in shapefiles (more on this below) based on some value that we care about. We'll take our inspiration from Plotly's gentle intro to [Choropleth maps](https://plotly.com/python/mapbox-county-choropleth/)\n",
    "\n",
    "The thing we want to look into is the SF police districts, shown below (image stolen from [this page](https://hoodline.com/2015/07/citywide-sfpd-redistricting-to-take-effect-sunday/)).\n",
    "\n",
    "![districts from web](https://raw.githubusercontent.com/suneman/socialdata2021/master/files/sfpdfinal.png)\n",
    "\n",
    "But because we are cool programmers, we want to create our own maps, **with our own information on them**. Let's do it!\n",
    "\n",
    "> *Exercise*: Let's plot a map with some random values in it.\n",
    ">\n",
    "> What we need to do to get going is to create some random data. Below is a little dictionary with a random value for each district that you can use if you want your plots to look like mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomdata = {'CENTRAL': 0.8903601342256143,\n",
    " 'SOUTHERN': 0.8642882941363439,\n",
    " 'BAYVIEW': 0.925634097746596,\n",
    " 'MISSION': 0.7369022697287458,\n",
    " 'PARK': 0.9864113307070926,\n",
    " 'RICHMOND': 0.5422239624697017,\n",
    " 'INGLESIDE': 0.5754056712571605,\n",
    " 'TARAVAL': 0.5834730737348696,\n",
    " 'NORTHERN': 0.08148199528212985,\n",
    " 'TENDERLOIN': 0.37014287986350447};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise* (continued):\n",
    ">\n",
    "> For this exercise, we'll use use the random values above and we'll also need some *shape-files*.\n",
    "> [Shapefiles can have many different formats](https://en.wikipedia.org/wiki/Shapefile). Because we are brilliant teachers and an all-round standup people, we are sharing the shapefiles as [`geojson`](https://en.wikipedia.org/wiki/GeoJSON), which is an easy-to-use format for shapefiles based on `json`.\n",
    ">\n",
    "> * Download the SFPD District shapefiles **[here](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/sfpd.geojson)**\n",
    "> * Now that you have the shapefiles, you can follow the example here: https://plotly.com/python/mapbox-county-choropleth/ but with the following modifications:\n",
    ">    * In the example the `id` is a so-called FIPS code. In our case the `id` is the `DISTRICT`\n",
    ">    * You will have to convert the dictionary of random values I included above to a Pandas dataframe with the right column headings.\n",
    ">    * The data used in the example has a range between zero and 12. Your data is between $[0,1]$. So you'll need to modify the plotting command to accound for that change.\n",
    ">    * You should also change the map to display the right zoom level.\n",
    ">    * And the map should center on San Francisco's `lat` and `lon`.\n",
    "> * Now you can create your map.\n",
    "\n",
    "Mine looks something like this.\n",
    "\n",
    "![map_example.png](https://raw.githubusercontent.com/suneman/socialdata2021/master/files/map_example.png)\n",
    "\n",
    "You're encouraged to play around with other settings, color schemes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise:* But it's crime-data. Let's do something useful and **visualize where it is safest to leave your car on a Sunday**.\n",
    "> \n",
    "> Take a moment to congratulate yourself. You now know how to create cool map plots!\n",
    "> * Now, we can focus on our main goal: *determine the districts where you should (and should not) leave your car on Sundays*. (Or stated differently, count up the number of thefts.)\n",
    "> * To do so, first:\n",
    ">  * Filter the crime dataset by the `DayOfWeek` category and also choose the appropriate crime category.\n",
    ">  * Aggregate data by police district.\n",
    "> * To create the plot, remember that your range of data-values is different from before, so you'll have to change the plotly command a bit.\n",
    "> * **Based on your map and analysis, where should you park the car for it to be safest on a Sunday? And where's the worst place?**\n",
    "> * Using visualizatios can help us uncover powerful data-patterns. However, when designing visualizations, we need to be aware of several illusions that can lead viewers to misinterpret the data we are showing (i.e. *perceptual errors*):\n",
    ">    * Try to change the range of data-values in the plot above. Is there a way to make the difference between district less evident? \n",
    ">    * Why do you think perceptual errors are a problem? Try to think of a few examples. You can have a look at this [article](https://www.businessinsider.com/fox-news-obamacare-chart-2014-3?r=US&IR=T) to get some inspiration.\n",
    "> * *Try this for Extra credit:*\n",
    ">     * Create plots for the same crime type, but different days, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
